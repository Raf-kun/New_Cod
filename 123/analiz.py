import os
import ast
import json
import subprocess
import chardet
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Union
import re

class UniversalRepoAnalyzer:
    def __init__(self, repo_url: str):
        self.repo_url = repo_url
        self.repo_name = repo_url.split("/")[-1].replace(".git", "")
        self.local_path = f"./temp_{self.repo_name}"
        self.dependency_graph = defaultdict(set)
        self.cyclic_dependencies = []
        self.file_types = defaultdict(int)
        self.binary_files = []

    def clone_repo(self) -> None:
        """–ö–ª–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π."""
        if not os.path.exists(self.local_path):
            subprocess.run(["git", "clone", self.repo_url, self.local_path], check=True)

    def scan_files(self) -> None:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏."""
        for root, _, files in os.walk(self.local_path):
            for file in files:
                file_path = os.path.join(root, file)
                self._analyze_file(file_path)

    def _analyze_file(self, file_path: str) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –µ–≥–æ —Ç–∏–ø–∞."""
        file_ext = os.path.splitext(file_path)[1].lower()
        self.file_types[file_ext] += 1

        if self._is_binary(file_path):
            self.binary_files.append(file_path)
            return

        try:
            with open(file_path, "rb") as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)["encoding"] or "utf-8"
                content = raw_data.decode(encoding)
        except UnicodeDecodeError:
            return

        relative_path = os.path.relpath(file_path, self.local_path)

        if file_ext == ".py":
            self._analyze_python(relative_path, content)
        elif file_ext == ".js":
            self._analyze_javascript(relative_path, content)
        elif file_ext == ".go":
            self._analyze_go(relative_path, content)
        elif file_ext == ".rs":
            self._analyze_rust(relative_path, content)
        elif file_ext in (".java", ".kt"):
            self._analyze_java(relative_path, content)
        elif file_ext == ".cpp":
            self._analyze_cpp(relative_path, content)

    def _is_binary(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –±–∏–Ω–∞—Ä–Ω—ã–º."""
        try:
            with open(file_path, "rb") as f:
                return b"\x00" in f.read(1024)
        except:
            return True

    def _analyze_python(self, file_path: str, content: str) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Python-—Ñ–∞–π–ª."""
        try:
            tree = ast.parse(content)
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    for alias in node.names:
                        module = alias.name.split(".")[0]
                        if not module.startswith((".", "_")) and module not in ("sys", "os"):
                            self.dependency_graph[file_path].add(module)
        except SyntaxError:
            pass

    def _analyze_javascript(self, file_path: str, content: str) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç JavaScript-—Ñ–∞–π–ª."""
        imports = re.findall(r"import\s+(?:.*?\s+from\s+)?[\"']([^\"']+)", content)
        for imp in imports:
            if not imp.startswith((".", "/")):
                self.dependency_graph[file_path].add(imp.split("/")[0])

    def _analyze_go(self, file_path: str, content: str) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Go-—Ñ–∞–π–ª."""
        imports = re.findall(r"import\s+\(([^)]+)\)", content, re.DOTALL)
        for imp_group in imports:
            for imp in imp_group.split("\n"):
                imp = imp.strip().strip('"')
                if imp and not imp.startswith((".", "_")):
                    self.dependency_graph[file_path].add(imp.split("/")[-1])

    def _analyze_rust(self, file_path: str, content: str) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Rust-—Ñ–∞–π–ª."""
        imports = re.findall(r"use\s+([^;]+);", content)
        for imp in imports:
            imp = imp.strip().split("::")[0]
            if imp and not imp.startswith((".", "crate", "self")):
                self.dependency_graph[file_path].add(imp)

    def detect_cycles(self) -> None:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ü–∏–∫–ª—ã –≤ –≥—Ä–∞—Ñ–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
        visited = set()
        recursion_stack = set()

        def dfs(node: str, path: List[str]) -> None:
            if node in recursion_stack:
                cycle_start_index = path.index(node)
                cycle = path[cycle_start_index:]
                if len(cycle) > 1:
                    self.cyclic_dependencies.append(tuple(cycle))
                return
            if node in visited:
                return

            visited.add(node)
            recursion_stack.add(node)
            path.append(node)

            for neighbor in self.dependency_graph.get(node, set()):
                dfs(neighbor, path.copy())

            recursion_stack.remove(node)
            path.pop()

        for node in self.dependency_graph:
            dfs(node, [])

    def export_to_dot(self, output_file: str = "dependencies.dot") -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –≤ —Ñ–æ—Ä–º–∞—Ç DOT."""
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("digraph Dependencies {\n")
            f.write('    rankdir="LR";\n')
            for src, deps in self.dependency_graph.items():
                for dst in deps:
                    f.write(f'    "{src}" -> "{dst}";\n')
            f.write("}\n")

    def export_to_json(self, output_file: str = "dependencies.json") -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –≤ JSON."""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —É–∑–ª—ã
        all_nodes = set(self.dependency_graph.keys())
        for deps in self.dependency_graph.values():
            all_nodes.update(deps)

        graph_data = {
            "nodes": [{"id": node} for node in all_nodes],
            "links": [
                {"source": src, "target": dst}
                for src, deps in self.dependency_graph.items()
                for dst in deps
            ],
            "cycles": [list(cycle) for cycle in self.cyclic_dependencies],
            "metadata": {
                "file_types": dict(self.file_types),
                "binary_files_count": len(self.binary_files)
            }
        }

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(graph_data, f, indent=2)

    def clean_up(self) -> None:
        """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É."""
        subprocess.run(["rm", "-rf", self.local_path], shell=True)

if __name__ == "__main__":
    repo_url = input("–í–≤–µ–¥–∏—Ç–µ URL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è: ").strip()
    analyzer = UniversalRepoAnalyzer(repo_url)
    
    print("üîÑ –ö–ª–æ–Ω–∏—Ä—É—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π...")
    analyzer.clone_repo()
    
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ñ–∞–π–ª—ã...")
    analyzer.scan_files()
    
    print("üîÑ –ò—â—É —Ü–∏–∫–ª—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
    analyzer.detect_cycles()
    
    print("üì§ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
    analyzer.export_to_dot()
    analyzer.export_to_json()
    
    analyzer.clean_up()
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"- –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (DOT): dependencies.dot")
    print(f"- –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (JSON): dependencies.json")
    if analyzer.cyclic_dependencies:
        print("‚ö† –ù–∞–π–¥–µ–Ω—ã —Ü–∏–∫–ª–∏—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
        for cycle in analyzer.cyclic_dependencies:
            print(" ‚Üí ".join(cycle))